{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Predictive Freight Volume Forecasting\n",
        "\n",
        "This project demonstrates time-series forecasting for freight volume prediction using machine learning. We generate synthetic freight data with seasonal patterns, economic indicators, and external factors, then train and evaluate multiple regression models to predict daily freight volumes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate Synthetic Freight Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "# Date range: 3 years of daily data (2022-2024)\n",
        "dates = pd.date_range(start='2022-01-01', end='2024-12-31', freq='D')\n",
        "n_days = len(dates)\n",
        "\n",
        "# Base volume with seasonal pattern (sine wave for seasonality)\n",
        "t = np.arange(n_days)\n",
        "seasonal = 500 * np.sin(2 * np.pi * t / 365)  # Annual seasonality\n",
        "\n",
        "# Weekly pattern (lower on weekends)\n",
        "day_of_week = dates.dayofweek\n",
        "weekly_pattern = np.where(day_of_week < 5, 0, -150)  # Weekend penalty\n",
        "\n",
        "# Upward trend\n",
        "trend = 2 * t / 365  # ~2 units per year growth\n",
        "\n",
        "# Random noise\n",
        "noise = np.random.normal(0, 80, n_days)\n",
        "\n",
        "# Base volume\n",
        "base_volume = 2000\n",
        "volume = base_volume + seasonal + weekly_pattern + trend + noise\n",
        "volume = np.maximum(volume, 100)  # Ensure positive volumes\n",
        "\n",
        "# Additional features\n",
        "fuel_price_index = 100 + np.cumsum(np.random.randn(n_days) * 0.5) + 0.1 * t / 365\n",
        "fuel_price_index = np.maximum(fuel_price_index, 80)\n",
        "\n",
        "economic_indicator = 100 + np.cumsum(np.random.randn(n_days) * 0.3) + 0.05 * t / 365\n",
        "economic_indicator = np.maximum(economic_indicator, 70)\n",
        "\n",
        "weather_severity = np.random.uniform(0, 10, n_days)\n",
        "\n",
        "# Holiday indicator (simplified: major US holidays)\n",
        "month_day = dates.strftime('%m-%d')\n",
        "holidays = ['01-01', '07-04', '12-25', '11-24', '11-25', '12-31']\n",
        "is_holiday = month_day.isin(holidays).astype(int)\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    'date': dates,\n",
        "    'volume': volume,\n",
        "    'fuel_price_index': fuel_price_index,\n",
        "    'economic_indicator': economic_indicator,\n",
        "    'weather_severity': weather_severity,\n",
        "    'day_of_week': day_of_week,\n",
        "    'month': dates.month,\n",
        "    'is_holiday': is_holiday\n",
        "})\n",
        "\n",
        "print(df.head())\n",
        "print()\n",
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# 1. Volume over time (line plot)\n",
        "axes[0, 0].plot(df['date'], df['volume'], linewidth=0.8, alpha=0.8)\n",
        "axes[0, 0].set_title('Freight Volume Over Time')\n",
        "axes[0, 0].set_xlabel('Date')\n",
        "axes[0, 0].set_ylabel('Volume')\n",
        "\n",
        "# 2. Monthly average volume (bar chart)\n",
        "monthly_avg = df.groupby('month')['volume'].mean()\n",
        "axes[0, 1].bar(monthly_avg.index, monthly_avg.values, color='steelblue', edgecolor='black')\n",
        "axes[0, 1].set_title('Monthly Average Volume')\n",
        "axes[0, 1].set_xlabel('Month')\n",
        "axes[0, 1].set_ylabel('Average Volume')\n",
        "\n",
        "# 3. Day-of-week pattern (box plot)\n",
        "df['day_name'] = df['date'].dt.day_name()\n",
        "day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
        "df_sorted = df.copy()\n",
        "df_sorted['day_name'] = pd.Categorical(df_sorted['day_name'], categories=day_order, ordered=True)\n",
        "df_sorted = df_sorted.sort_values('day_name')\n",
        "sns.boxplot(data=df_sorted, x='day_name', y='volume', ax=axes[1, 0])\n",
        "axes[1, 0].set_title('Volume by Day of Week')\n",
        "axes[1, 0].set_xlabel('Day of Week')\n",
        "axes[1, 0].set_ylabel('Volume')\n",
        "axes[1, 0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# 4. Volume distribution (histogram)\n",
        "axes[1, 1].hist(df['volume'], bins=50, edgecolor='black', alpha=0.7)\n",
        "axes[1, 1].set_title('Volume Distribution')\n",
        "axes[1, 1].set_xlabel('Volume')\n",
        "axes[1, 1].set_ylabel('Frequency')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('eda_plots.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print('Shape before feature engineering:', df.shape)\n",
        "\n",
        "# Lag features (7, 14, 28 days)\n",
        "for lag in [7, 14, 28]:\n",
        "    df[f'volume_lag_{lag}'] = df['volume'].shift(lag)\n",
        "\n",
        "# Rolling means (7, 14, 30 days)\n",
        "for window in [7, 14, 30]:\n",
        "    df[f'volume_rolling_mean_{window}'] = df['volume'].rolling(window=window).mean()\n",
        "\n",
        "# Rolling std (7 days)\n",
        "df['volume_rolling_std_7'] = df['volume'].rolling(window=7).std()\n",
        "\n",
        "# Month sin/cos encoding\n",
        "df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
        "df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
        "\n",
        "# Drop NaN rows from lag/rolling features\n",
        "df = df.dropna()\n",
        "\n",
        "print('Shape after feature engineering:', df.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Training & Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Train/test split (last 90 days as test)\n",
        "test_size = 90\n",
        "train_df = df.iloc[:-test_size]\n",
        "test_df = df.iloc[-test_size:]\n",
        "\n",
        "# Feature list (exclude date, volume)\n",
        "exclude_cols = ['date', 'volume', 'day_name']\n",
        "feature_cols = [c for c in df.columns if c not in exclude_cols]\n",
        "\n",
        "X_train = train_df[feature_cols]\n",
        "y_train = train_df['volume']\n",
        "X_test = test_df[feature_cols]\n",
        "y_test = test_df['volume']\n",
        "\n",
        "# Train models\n",
        "models = {\n",
        "    'Linear Regression': LinearRegression(),\n",
        "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
        "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
        "}\n",
        "\n",
        "results = {}\n",
        "predictions = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    predictions[name] = y_pred\n",
        "    \n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
        "    \n",
        "    results[name] = {'MAE': mae, 'RMSE': rmse, 'R²': r2, 'MAPE (%)': mape}\n",
        "\n",
        "# Print comparison table\n",
        "results_df = pd.DataFrame(results).T\n",
        "print(results_df.round(4))\n",
        "\n",
        "# Store best model name for later use\n",
        "best_model_name = min(results, key=lambda k: results[k]['RMSE'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Predictions vs Actuals"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "y_pred_best = predictions[best_model_name]\n",
        "residuals = y_test.values - y_pred_best\n",
        "residual_std = np.std(residuals)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "ax.plot(test_df['date'].values, y_test.values, label='Actual', color='blue', linewidth=2)\n",
        "ax.plot(test_df['date'].values, y_pred_best, label=f'Predicted ({best_model_name})', color='orange', linewidth=1.5, linestyle='--')\n",
        "ax.fill_between(test_df['date'].values, y_pred_best - residual_std, y_pred_best + residual_std, alpha=0.3, color='orange', label='±1 Std Confidence')\n",
        "ax.set_title('Predictions vs Actuals (Test Set)')\n",
        "ax.set_xlabel('Date')\n",
        "ax.set_ylabel('Volume')\n",
        "ax.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('predictions_vs_actuals.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Importance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Use best tree-based model (Random Forest or Gradient Boosting)\n",
        "tree_models = ['Random Forest', 'Gradient Boosting']\n",
        "best_tree = best_model_name if best_model_name in tree_models else 'Gradient Boosting'\n",
        "model_for_importance = models[best_tree]\n",
        "\n",
        "if hasattr(model_for_importance, 'feature_importances_'):\n",
        "    importance = pd.DataFrame({\n",
        "        'feature': feature_cols,\n",
        "        'importance': model_for_importance.feature_importances_\n",
        "    }).sort_values('importance', ascending=True)\n",
        "    top15 = importance.tail(15)\n",
        "    \n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.barh(top15['feature'], top15['importance'], color='steelblue', edgecolor='black')\n",
        "    plt.xlabel('Importance')\n",
        "    plt.title(f'Top 15 Feature Importance ({best_tree})')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('feature_importance.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "else:\n",
        "    print('Linear Regression does not have feature_importances_. Using Gradient Boosting for importance plot.')\n",
        "    gb = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
        "    gb.fit(X_train, y_train)\n",
        "    importance = pd.DataFrame({'feature': feature_cols, 'importance': gb.feature_importances_}).sort_values('importance', ascending=True)\n",
        "    top15 = importance.tail(15)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.barh(top15['feature'], top15['importance'], color='steelblue', edgecolor='black')\n",
        "    plt.xlabel('Importance')\n",
        "    plt.title('Top 15 Feature Importance (Gradient Boosting)')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('feature_importance.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusions\n",
        "\n",
        "- **Model Performance**: The best-performing model (based on RMSE) provides reliable freight volume predictions. Tree-based models (Random Forest, Gradient Boosting) typically outperform linear regression when capturing non-linear relationships and interactions.\n",
        "\n",
        "- **Key Drivers**: Lag features (recent volume history) and rolling statistics are among the most important predictors. Seasonal patterns (month encoding) and day-of-week effects also contribute significantly to forecast accuracy.\n",
        "\n",
        "- **Recommendations**: Consider retraining models periodically as new data becomes available. Explore additional features such as weather forecasts, economic indicators, and supply chain events. For production deployment, implement proper train/validation/test splits and cross-validation to avoid overfitting."
      ]
    }
  ]
}
